defaults:
  - common
  - model: ours
  - env : maze

dataset_cls : LVD.data.Maze_Dataset_Div
structure : gc_div_joint

# env:
#   epoch_cycles_train: ${get_cycle:${env.batch_size}}

model:
  state_encoder :
    in_feature : ${env.state_dim} 
    out_dim : ${env.latent_state_dim} 
  
  state_decoder :
    in_feature : ${env.latent_state_dim}
    out_dim : ${env.state_dim}

  dynamics :
    in_feature : ${add:${env.latent_state_dim}, ${env.skill_dim}}
    out_dim : ${env.latent_state_dim} 

  inverse_dynamics :
    in_feature : ${multiply:${env.latent_state_dim}, 2}
    out_dim : ${multiply:${env.skill_dim}, 2}

  subgoal_generator :
    in_feature : ${add:${env.latent_state_dim}, ${env.n_goal}} 
    out_dim : ${env.latent_state_dim}

  prior:
    in_feature : ${env.latent_state_dim} 
    out_dim : ${multiply:${env.skill_dim}, 2}

  skill_encoder:
    in_feature: ${add:${env.state_dim}, ${env.action_dim}}
    out_dim : ${multiply:${env.skill_dim}, 2}

  skill_decoder:
    in_feature : ${add:${env.state_dim}, ${env.skill_dim}} 
    out_dim : ${env.action_dim} 
    state_dim : ${env.state_dim}
    z_dim  : ${env.skill_dim}

  q_function:
    in_feature: ${add:${env.latent_state_dim}, ${env.n_goal}, ${env.skill_dim}} 

rl:
  render_period : 5
  q_warmup : 5_000 
  qf_lr : 3e-4
  precollect : 20
  norm : false
  rl_batch_size : 256