defaults:
  - common
  - model: gc_skimo
  - env : kitchen

dataset_cls : LVD.data.Kitchen_Dataset
structure : skimo

# n_skill : 3
scheduler_params:
  min_lr: 1e-6

env:
  reg_beta : 1e-4
  auto_alpha: False
  init_alpha : 0.01
  policy_lr : 3e-5      # for few-shot adaptation by RL loss
  gcsl_lr : 1e-6        # gcsl lr 
  consistency_lr : 5e-7 # for submodule adaptation 
  
model:
  state_encoder :
    in_feature : ${env.state_dim} 
    latent_state_dim : ${env.latent_state_dim}
    env_state_dim : ${env.n_env}
    ppc_state_dim : ${add:${env.n_pos}, ${env.n_nonPos}}
    pos_state_dim : ${env.n_pos}
    nonPos_state_dim : ${env.n_nonPos}
    out_dim : ${env.latent_state_dim}
  
  state_decoder :
    in_feature : ${env.latent_state_dim}
    latent_state_dim : ${env.latent_state_dim}
    env_state_dim : ${env.n_env}
    ppc_state_dim : ${add:${env.n_pos}, ${env.n_nonPos}}
    pos_state_dim : ${env.n_pos}
    nonPos_state_dim : ${env.n_nonPos}
    out_dim : ${env.state_dim}

  dynamics: 
    in_feature : ${add:${env.latent_state_dim}, ${env.skill_dim}}
    out_dim : ${env.latent_state_dim}

  reward_function:
    in_feature : ${add:${env.latent_state_dim}, ${env.skill_dim}, ${env.n_goal}}
    out_dim : 1

  prior:
    in_feature : ${env.n_pos}
    out_dim : ${multiply:${env.skill_dim}, 2}

  skill_encoder:
    in_feature: ${add:${env.state_dim}, ${env.action_dim}}
    out_dim : ${multiply:${env.skill_dim}, 2}

  skill_decoder:
    in_feature : ${add:${env.n_pos}, ${env.skill_dim}} 
    out_dim : ${env.action_dim} 
    state_dim : ${env.n_pos}
    z_dim  : ${env.skill_dim}


  high_policy:
    in_feature : ${add:${env.latent_state_dim}, ${env.n_goal}} 
    out_dim : ${multiply:${env.skill_dim}, 2}

  q_function:
    in_feature: ${add:${env.latent_state_dim}, ${env.n_goal}, ${env.skill_dim}} 