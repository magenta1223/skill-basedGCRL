# ------------------ Env Specification ------------------ #
env_name : maze
max_seq_len : 500
envtask_cfg : LVD.envs.Maze_EnvTaskConfig

# task type
manipulation : false

# dimension
state_dim : 4
n_pos : 2      # x, y location 
n_nonPos : 2   # x, y velocity 
n_env : 0      #   
n_goal : 2     # x, y location
action_dim : 2 # x, y acceleration 

# pretrain params
epoch_cycles_train : 320 # 1epoch = 100 iteration of dataset 
batch_size : 1024 
epochs : 100
warmup_steps : 50
val_data_size : 5000
only_last : false        # for hindsight relabeling 
sample_interval : 5

# online adaptation params
time_limit : 3000      # maximum horizon in target environment 
buffer_size : 20000    # maximum buffer size 
n_episode : 50         # maximum episode in few-shot adaptation 
reuse_rate : 512       # sample reuse rate 
step_per_ep : 400      # not used. deprecated.
precollect : 0         # precollect to initialize the buffer 
early_stop_rwd : 80    # early stop reward. (equivalent to success rate 80%)
max_reward : 100       # 

# ------------------ Few-shot adaptation ------------------ #
# value warmup 
q_warmup : 2_000       # update step for warminup value function 
consistency_lr : 3e-5  # update submodules
gcsl_lr: 3e-5          # update policy by GCSL loss  
policy_lr : 1e-7       # update policy by RL loss  

# ------------------ WGCSL ------------------ #
value_lr : 3e-5           # value function lr in few-shot 
adv_clip : 10             # advantage clip thershold M for GEAW
distance_reward : true    # Reward relabeling
reward_threshold : 3      # Reward relabeling
baw_max : 20              # best advantage threshold for BAW(Best-Advantage-Weight) in WGCSL

# ------------------ SPiRL, SkiMo ------------------ #
subseq_len : 11           # skill horizon + 1
reg_beta : 5e-4           # skill space regulrization beta 
skill_dim : 10            # skill dimension
prior_state_dim : 4       # not used.

# alpha 
init_alpha : 1e-4         # policy regularization alpha 
increasing_alpha : false  # allow alpha only incresing while few-shot adaptation. 
auto_alpha : false        # alpha learning indicator 

# skimo 
latent_state_dim : 256

# ------------------ Ours ------------------ # 
offline_buffer_size : 10000 # 10000
std_factor : 1.0
# discount : true
# discount_value : 0.99
mixin_start : 1
goal_factor : 1 # 1
quantile : 0.6 #0.9 # 0~1

log_score : true
truncated : true
norm_G: true 

disc_pretrain :
  apply: true
  start : 0.99 # 0.99
  end : 0.99
  epochs : 80 #40
  static : true # true 
  

mixin_ratio :               # generated traj mixin ratio 
  start : 0.075 # 0.05
  end : 0.075 #0.1
  epochs : 50 #40
  static : true # false 

plan_H : 1000

rollout:
  static : true
  start : 1000
  end : 1000
  epochs : 50

invD_lr : 5e-7
D_lr : 5e-7
f_lr : 5e-7


# ablation params
skill_len : 10


# for instant test 
instant:
  one: 1
  two: 2 
  three : 3

target_kl_start : 15
target_kl_end : 15
