# ENVIRONMENT # 
env_name : maze
max_seq_len : 500
envtask_cfg : LVD.envs.Maze_EnvTaskConfig

# task type
manipulation : false

# reward type
binary_reward : false

# input control 
norm_G : true
normalize_state : false

# RND filter threshold 
std_factor : 1.1

# discount for generated traj 
discount : true

# flat_Dtest 
testtest : false

# WGCSL
value_lr : 3e-5


# Skimo
# Reward relabeling threshold in offline WGCSL / RIS
# Reward relabeling threshold in offline WGCSL / RIS
reward_threshold : 3
# maximum value of best advantage threshold for BAW(Best-Advantage-Weight) in WGCSL
baw_max : 20



# dimension
state_dim : 4
n_pos : 2
n_nonPos : 2
n_env : 0
n_goal : 2
action_dim : 2
skill_dim : 10
latent_state_dim : 256

# pretrain params
epoch_cycles_train : 320
offline_buffer_size : 10000
batch_size : 1024
epochs : 140
warmup_steps : 50

# model-based skill augmentation params
mixin_start : 1
mixin_ratio : 0.05
plan_H : 1000

# SPiRL params
subseq_len : 11
reg_beta : 5e-4
prior_state_dim : 4
val_data_size : 5000

# online adaptation params
time_limit : 3000
target_kl_start : 15
target_kl_end : 15
init_alpha : 1e-4
increasing_alpha : false
auto_alpha : false # skimo, spirlÏùÄ true
reuse_rate : 512
step_per_ep : 400
q_warmup : 5000
q_weight : 1
precollect : 10
early_stop_threshold : ${multiply:100, 0.8}
max_reward : 100
use_hidden : true
consistency_lr : 3e-5
gcsl_lr: 3e-5
buffer_size : 20000
n_episode : 50
kl_decay : 0.99
policy_lr : 1e-7
early_stop_rwd : 80


invD_lr : 5e-7
D_lr : 5e-7
f_lr : 5e-7


# ablation params
skill_len : 10