# ENVIRONMENT # 
env_name : kitchen
env_name_offline : kitchen-mixed-v0
envtask_cfg : LVD.envs.Kitchen_EnvTaskConfig
max_seq_len : 280

# task type 
manipulation : true

# reward type
binary_reward : false

# input control 
norm_G : true
normalize_state : false

# RND filter threshold 
std_factor : 1.1 # 1.35

# discount for generated traj 
discount : false

# flat_Dtest 
testtest : false

# Skimo

# Reward relabeling threshold in offline WGCSL / RIS
reward_threshold : 3
# best advantage threshold for BAW(Best-Advantage-Weight) in WGCSL
baw_max : 80

# dimension
state_dim : 30
n_pos : 9 # proprioceptive 
n_nonPos : 0 
n_env : 21 # object  
n_goal : 30
action_dim : 9
skill_dim : 10
latent_state_dim : 256

# pretrain params
epoch_cycles_train : 100
offline_buffer_size : 10000
batch_size : 1024
epochs : 100
warmup_steps : 10

# model-based skill augmentation params
mixin_start : 1
mixin_ratio : 0.1
plan_H : 160

# SPiRL params
subseq_len : 11
reg_beta : 0.0005
prior_state_dim : 30
val_data_size : 5000

# online adaptation params
time_limit : 280
target_kl_start : 50
target_kl_end : 15
init_alpha : 0.005
increasing_alpha : false
auto_alpha : false
reuse_rate : 4000
step_per_ep : 100
q_warmup : 10_000
q_weight : 1
precollect : 20
early_stop_threshold : ${multiply:4, 0.8}
max_reward : 4
use_hidden : true # use latent state for Q 
consistency_lr : 3e-5
buffer_size : 20000
n_episode : 150
kl_decay : 0.99
early_stop_rwd : 3.5
# min_lr * 0.05
policy_lr : 5e-7
# policy_lr : 3e-5



# invD_lr : 1e-6
# D_lr : 1e-5
# f_lr : 1e-6

# min_lr * 0.05
invD_lr : 5e-7
D_lr : 5e-7
f_lr : 5e-7