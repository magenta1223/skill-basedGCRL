defaults:
  - common
  - model: gc_skimo
  - env : maze

dataset_cls : LVD.data.maze.maze_dataloader.Maze_Dataset
structure : gc_skimo

# n_skill : 10
scheduler_params:
  min_lr: 1e-6

env:
  reg_beta : 1e-4
  auto_alpha : true
  init_alpha : 0.01  
  policy_lr : 3e-5        # for few-shot adaptation by RL loss
  gcsl_lr : 1e-6          # gcsl lr 
  consistency_lr : 5e-7   # for submodule adaptation 

model:
  state_encoder :
    in_feature : ${env.state_dim} 
    latent_state_dim : ${env.latent_state_dim}
    env_state_dim : 2
    ppc_state_dim : ${env.n_pos}
    pos_state_dim : ${env.n_pos}
    nonPos_state_dim : ${env.n_nonPos}
    out_dim : ${env.latent_state_dim}
  
  state_decoder :
    in_feature : ${env.latent_state_dim}
    latent_state_dim : ${env.latent_state_dim}
    env_state_dim : 2
    ppc_state_dim : ${env.n_pos}
    pos_state_dim : ${env.n_pos}
    nonPos_state_dim : ${env.n_nonPos}
    out_dim : ${env.state_dim}

  dynamics: 
    in_feature : ${add:${env.latent_state_dim}, ${env.skill_dim}}
    out_dim : ${env.latent_state_dim}

  reward_function:
    in_feature : ${add:${env.latent_state_dim},  ${env.n_goal}, ${env.skill_dim}}
    out_dim : 1

  prior:
    in_feature : ${env.state_dim}
    out_dim : ${multiply:${env.skill_dim}, 2}

  skill_encoder:
    in_feature: ${add:${env.state_dim}, ${env.action_dim}}
    out_dim : ${multiply:${env.skill_dim}, 2}

  skill_decoder:
    in_feature : ${add:${env.state_dim}, ${env.skill_dim}}
    out_dim : ${env.action_dim} 
    state_dim : ${env.state_dim}
    z_dim  : ${env.skill_dim}


  high_policy:
    in_feature : ${add:${env.latent_state_dim}, ${env.n_goal}} 
    out_dim : ${multiply:${env.skill_dim}, 2}

  q_function:
    in_feature: ${add:${env.latent_state_dim}, ${env.n_goal}, ${env.skill_dim}} 
